[
{
	"uri": "/creating_application/workflows/ci_pipelines/",
	"title": "CI Pipeline",
	"tags": null,
	"description": "",
	"content": "Create CI Pipeline    After providing the name to the workflow, click on Add CI Pipeline to add a new CI Pipeline. When you click on Add CI pipeline, you can see three options\n  Continuous Integration\n  Linked CI Pipeline\n  Incoming Webhook\n     1. Continuous Integration CI related configuration is provided under this section. We will later discuss this in detail.\nClick on Continuous Integration to configure CI Pipeline, a new window will appear. You can see 5 options on this new window:\n   Key Description     Pipeline Name Name of the pipeline   Source Type Select the source through which the CI Pipeline will be triggered by Automatic or Manual.   Branch Name/Tag Regex Enter branch name/tag regex.   Stages 1. Pre-build Stages- Scripts to be executed before building an image. 2. Docker build Stages- Provide a new argument and override an old argument in key-value pair. 3. Docker build Stages- Scripts to be executed after building image   Scan for vulnerabilities It will scan your image and find if any vulnerabilities present        1. Pipeline Name First of all, give a name to your CI pipeline.\n2. Source Type In source type, select between Automatic or Manual. Here if you select Automatic, the CI pipeline will be triggered automatically whenever you push any commit to your repository. But if you select Manual, then you have to trigger your CI pipeline every time manually through the console.\n3. Branch Name In this column, you have to provide your branch or tag regex where your code is present and the branch to want this CI pipeline to be linked with.\nExample- It can be a master branch in the production and development/staging branch in the dev environment. But the branch name completely depends on the user, which branch name the user wants to provide here.\n4. Stages (a) Pre-build This section is used for those Scripts which you want to execute before building the Docker image. To add a Pre-build stage, click on Add Stage and provide a name to your pre-stage and write your script as per your requirement. This will run in sequence before the docker image is built. You can also provide the path of the directory where the output of the script will be stored in the Remote Directory column. But this is optional to fill because many times you run scripts which do not provide any output.\nYou can add one stage or more than one stage in a CI Pipeline.\n    (b) Docker build Though we have the option available in Docker build configuration section to add arguments in key-value pairs for docker build image. But one can also provide docker build arguments here as well. This is useful, in case you want to override them or want to add new arguments to build your docker image.\n(c) Post-build The post-build stage is similar to the pre-build stage. The difference between the post-build stage and the pre-build stage is that the post-build will run when your CI pipeline will be executed successfully.\nAdding a post-build stage is similar to adding a pre-build stage. Click on Add Stage and provide a name to your post-stage. Here you can write your script as per your requirement, which will run in sequence after the docker image is built. You can also provide the path of the directory in which the output of the script will be stored in the Remote Directory column. And this is optional to fill because many times you run scripts which do not provide any output.\n    NOTE: You can provide pre-build and post-build stages via the Devtron tool’s console or can also provide these details by creating a file “devtron.ci.yaml” inside your repository. There is a pre-defined format to write this file. And we will run these stages using this YAML file.\nYou can provide some stages on the Devtron tool’s console and some stages in the devtron.ci.yaml file. But stages defined through the Devtron dashboard are first executed then the stages defined in the devtron.ci.yaml file.\n5. Scan for vulnerabilities Scan for vulnerabilities adds a security feature in your application. If you enable this option, your code will be scanned for any vulnerabilities present in your code. And you will be informed about these vulnerabilities. For more details please check doc\nYou have provided all the details required to create a CI pipeline, now click on Create Pipeline.\nUpdate CI Pipeline You can also update any configuration of an already created CI Pipeline, except the pipeline name. The pipeline name can not be edited.\n    Click on your CI Pipeline, to update your CI Pipeline. A window will be popped up with all the details of the current pipeline.\n    Make your changes and click on Update Pipeline to update your Pipeline.\nDelete CI Pipeline You can only delete CI Pipeline if you have no CD Pipeline created in your workflow.\nTo Delete a CI Pipeline, go to the App Configurations and then click on Workflow editor\n    Click on Delete Pipeline to delete the CD Pipeline\nAutomated Test suite integration in the CI step using devtron-ci.yaml Users can run the Test case using the Devtron Dashboard or by including the Test cases in the devtron.ci.yaml file in the source git repository. For reference, check: https://github.com/kumarnishant/getting-started-nodejs/blob/master/devtron-ci.yaml\nThe test cases given in the script will run before the Test Cases given in the devtron.ci.yaml\n       Field Description     version specify the version of yaml   appliesTo applies the changes to a specified branch   type branch type on which changes are to be applied, it can be BRANCH_FIXED or TAG_PATTERN   value branch name on which changes are to be applied, it can take a value as the name of branch (“master”) or as a regular expression (\u0026quot;%d.%d.%d-rc\u0026quot;)   script A script which you want to execute, you can also execute the docker commands here   beforeDockerBuildStages script to run before the docker build step   afterDockerBuildStages script to run after the docker build step   outputLocation The location where you want to see the output of the report of Test cases    External CI Pipeline You can use Devtron for Deployments on Kubernetes while still using your own CI tool such as Jenkins. External CI features can be used for such cases where the CI tool is hosted outside the Devtron platform.\n    You can send the ‘Payload script’ to your CI tools such as Jenkins and Devtron will receive the build image every time the CI Service is triggered or you can use the Webhook URL which will build an image every time CI Service is triggered using Devtron Dashboard.\n   Key Description     Pipeline Name Name of the pipeline   Source Type ‘Branch Fixed’ or ‘Tag Regex’   Branch Name Name of the branch    Linked CI Pipeline If one code is shared across multiple applications, Linked CI Pipeline can be used, and only one image will be built for multiple applications because if there is only one build, it is not advisable to create multiple CI Pipelines.\n   "
},
{
	"uri": "/creating_application/",
	"title": "Creating Application",
	"tags": null,
	"description": "",
	"content": "Welcome! This is the documentation for Creating Applications\n  Parts of Documentation\nGit Material\nDocker Configuration\nDeployment Template\nWorkflows\nConfig Maps\nSecrets\nEnvironment Overrides "
},
{
	"uri": "/use_cases/devtron_genric_charts_to_run_cron_jobs_or_one_time_job/",
	"title": "Devtron Generic Helm Chart To Run Cron Job Or One Time Job",
	"tags": null,
	"description": "",
	"content": "Using Devtron-generic-Helm Chart to run Cron Job or One Time job You can discover over 200 Charts from the Devtron Chart store to perform different tasks such as to deploy a YAML file.\nYou can use Devtron-generic-Helm Chart to run the cron jobs or one time job.\nSelect the Devtron-generic Helm Chart from the Devtron Chart Store.\n    Select the Chart Version and the Chart Value of the Chart.\nAnd, then Click on Deploy\n    Configure Devtron-generic-helm chart\n    Click on Deploy Chart\n   Key Description     App Name Name of the app   Project Name of the Project   Environment Select the Environment in which you want to deploy app   Chart Version Select the Version of the chart   Chart Values Select the Chart Value or Create a Custom Value    In values.yaml, you can specify the YAML file that schedules the cron job for your application.\n"
},
{
	"uri": "/global_configurations/git_accounts/",
	"title": "Git Accounts",
	"tags": null,
	"description": "",
	"content": "Glocal configurations are used to configure your Git Providers, Docker Registry, Kubernetes clusters, different environments, User Management, and different Notifications for your application. We have segregated all the configurations and we will discuss how to configure all these one by one.\nGit Account Configuration Global Configuration helps you add a Git provider. Click on Add git account to add a new git provider and provide three inputs as below.\n  Name\n  URL\n  Authentication type\n    Anonymous\n  Password/Auth token\n  User auth\n      1. Name Provide a name to your Git provider. This will be added as one option in the Git Provider drop-down inside the Git Material section.\n2. URL Provide the URL of your Version Controller. For example- github.com for Github, https://gitlab.com for GitLab, etc.\n3. Authentication type Here you have to provide the type of Authentication required by your version controller. We are supporting three types of authentications, you can choose any one from them.\n Anonymous  If you select Anonymous then you do not have to provide any username, password, and authentication token. Just click on Save to save your git account provider details.\n     Password/Auth token  If you select Password/Auth token then you have to provide the Access token for the authentication of your version controller account inside the Access token box. Click on Save to save your git account provider details.\n     User auth  If you choose User auth then you have to provide the Username and Password of your version controller account. Click on Save to save your git account provider details.\n    Update Git Account You can update your saved git account settings at any point in time. Click on the git account which you want to update. Make changes and click on Update to save you changes.\n    Note: You can enable and disable your git account setting. If you enable it, then you are able to see the git account in the drop-down of Git provider.    "
},
{
	"uri": "/creating_application/git_material/",
	"title": "Git Material",
	"tags": null,
	"description": "",
	"content": "Git material is used to pull the application source code during the CI step. Inside git material when you click on “Add material” you will see three options as shown below:\n  Select Provider\n  Git Repo URL\n  Checkout Path\n   Devtron also supports multiple git repositories in a single deployment. We will discuss this in detail in the multi git option below.\n  1. Select Provider In the provider section, you have to select the git provider of your code repository like- Github, Gitlab, and Bitbucket, etc.You can configure your git provider via global configuration.\n 2. Git Repo URL Inside the git repo URL, you have to provide your code repository’s https URL. For Example- https://github.com/Username/Repo_name.git\n    3. Checkout Path The git checkout path is the directory where your code is pulled or cloned for the repository provided in the previous step.\nThis field is optional in case of a single git repository application and you can leave the path as default. The default value of this field is ./\nBut if you want to go with a multi git approach then you can leave the path as default for one of the repository but for other repositories you have to provide this path. In multi git checkout, this checkout path becomes mandatory for other repositories\n    4. Multi Git: As we discussed, Devtron also supports multiple git repositories in a single application. To add multiple repositories, click on add material again and follow the steps from 1 to 3 again.\nPlease note even though you can add multiple repositories only one image will be created based on the docker file as shown in the docker build config.\n Why do we need MultiGit support-\n Let’s try to understand this with an example. Due to security reasons, you may want to keep sensitive configuration like third party API keys in separate access restricted git repositories and source code in a git repository on which every developer has access. To deploy this application both repositories code is required and multi-git will help you to do that.\nProvide a different checkout path for both the repositories, if different repositories are provided the same checkout path then files from different repositories can overwrite each other. These checkout paths will be used by the docker file to create a docker image.\nIf any change is pushed to the configured repositories then the CI will be triggered and the image file will be built based on the latest commits of the configured repositories and pushed to the docker registry.\nFew other examples, where you may want to have multiple repositories for your application and will need multi git checkout support\n  To make code modularize, you are keeping front-end and back-end code in different repositories.\n  Due to security reasons you are keeping configuration files in different access restricted git repositories.\n  Common Library extracted out in different repo so that it can be used via multiple other projects.\n  "
},
{
	"uri": "/deploy_chart/overview/",
	"title": "Overview Of Charts",
	"tags": null,
	"description": "",
	"content": "Deploying Charts Charts can be deployed individually or by creating a group of Charts.\nBoth methods are mentioned in the given document.\nDeploying Chart     Select Discover and then select the chart that you want to use\nClick on README.MD to get more idea about the configurations of the chart\n  Select the Chart Version that you want to use and Chart Value, you can either use the default Values or custom values.\nTo know about Custom Values, Click On: Custom Values\nThe configuration values can be edited in the section given below Chart Version.\n   Key Description     App Name The name of the app   Project Project of the app   Environment Environment of the app to be deployed   Chart Version Version of the chart to be used        ReadMe.md present on the left can be used by the user to set configuration values\n   Click on Deploy Chart to deploy the chart\n    You can see the status of the chart deployed. Click on Values.Yaml to reconfigure the deployment.\n    Configuration values can be edited over here by the help of ReadMe.md\nSelect Update And Deploy to update new settings.\nCustom Values You can use the default values or create Custom value by clicking on  Create Custom\n    You can name your Custom Value, select the Chart Version and change the configurations in YAML file.\n    Click on Save Template to save the configurations.\nDeploying Chart Groups To deploy multiple applications and work with them simulataneously, you can use Chart Groups.\nTo create Chart Groups Click on Discover and Click on Create Group\n    Add the Group Name and Description and Click on Create Group\n    You can select the Charts that you want to add in your Chart Group by clicking on \u0026lsquo;+\u0026rsquo; sign. You can select a particular chart multiple number of times according to your requirements.\n   Select the Version and Values for your charts.\nYou can use Default Values or the Custom Values, just make sure the Value that you select for your Chart is comptabile with the Version of the Chart that you are using.\n    To edit the Chart Configuration, Click on \u0026lsquo;Edit\u0026rsquo;\n    You can Add more Charts to your existing Chart Group or Delete Charts from your existing Chart Group.\nAfter making changes, Click on Save to save changes to your Chart Group.\n    If you wish to edit the Chart Configuration, Double Click on that Chart and edit the Configurations in YAML File.\n    You can edit the App Name, Chart Version, Values, Deploy Environment and the YAML file.\n    Key Description     App Name Name of the app   Project Name of Project in which app has to be created   Environment Name of the Environment in which app has to be deployed   Chart Version Select the Version of the chart to be used    Click on Deploy to initiate the deployment of a Chart in Chart Group.\n"
},
{
	"uri": "/deploying_applications/triggering_ci/",
	"title": "Triggering CI",
	"tags": null,
	"description": "",
	"content": "Triggering CI Pipelines     The CI Pipeline can be triggered by selecting Select Material\nCI Pipelines that are set as Automatic are always triggered as soon as a new commit is made to the git branch they\u0026rsquo;re sensing. However, CI Pipelines can always be manually triggered as and if required.\n    Various commits done in the repository can be seen, here along with details like Author, Date etc. Select the commit that you want to trigger and then select \u0026ldquo;Start Build\u0026rdquo; to trigger the CI Pipeline.\nRefresh icon, refreshes Git Commits in the CI Pipeline and fetches the latest commits from the “Repository”\nIgnore Cache : This option will ignore the previous build cache and create a fresh build. If selected, will take a longer build time than usual.\n    It can be seen that the pipeline is triggered here and is the Running state.\nClick on your CI Pipeline or Click on Build History to get the details about the CI Pipeline such as logs, reports etc.\n    You can read the logs of the CI Pipeline from here.\n    Click on Source code to view the details such as commit id, Author and commit message of the Git Material that you have selected for the build.\n   Click on Artifacts to download the reports of the Pre-CI and Post-CI stages if any.\n"
},
{
	"uri": "/creating_application/workflows/cd_pipelines/",
	"title": "CD Pipeline",
	"tags": null,
	"description": "",
	"content": "Create CD Pipeline    Click on “+” sign on CI Pipeline to attach a CD Pipeline to it.\n   One can have a single CD pipeline or multiple CD pipelines connected to the same CI Pipeline. A CD pipeline corresponds to one environment or in other words, an environment of an application can have only one CD pipeline. So images created by the CI pipeline can be deployed into multiple environments.\nIf you already have one CD pipeline and want to add more, you can add them by clicking on the “+” sign and selecting the environment, where you want to deploy this application to. A new CD Pipeline will be created for the environment you selected.\nCD pipeline configuration has below options to be configured:\n   Key Description     Pipeline Name Enter the name of the pipeline to be created   Environment Select the environment in which you want to deploy   Pre-deployment stage Run any configuration and provide secrets before the deployment   Deployment stage Select the deployment type through which the CD Pipeline will be triggered by Automatic or Manual.   Deployment Strategy Select the type of deployment strategy that you want to enable by clicking “Add Deployment Strategy”   Post-deployment stage Run any configuration and provide secrets after the deployment        1. Pipeline Name Inside the Pipeline name column, give a name to your Continuous deployment as per your understanding.\n2. Deploy to Environment Select the environment where you want to deploy your application. Once you select the environment, it will display the Namespace corresponding to your selected environment automatically.\n3. Pre-deployment Stage Sometimes you encounter a requirement where you have to Configure actions like DB migration, which you want to run before the deployment. The Pre-deployment Stage comes into the picture in such scenarios.\nPre-deployment stages can be configured to be executed automatically or manually.\nIf you select automatic, Pre-deployment Stage will be triggered automatically after the CI pipeline gets executed and before the CD pipeline starts executing itself. But, if you select a manual, then you have to trigger your stage via console.\nAnd if you want to use some configuration files and secrets in pre-deployment stages or post-deployment stages, then you can make use of Config Maps \u0026amp; Secrets options.\nConfig Maps can be used to define configuration files. And Secrets can be defined to keep the secret data of your application.\nOnce you are done defining Config Maps \u0026amp; Secrets, you will get them as a drop-down in the pre-deployment stage and you can select them as part of your pre-deployment stage.\nThese Pre-deployment CD / Post-deployment CD pods can be created in the deployment cluster or in the devtron build cluster. Running these pods in a Deployment cluster is recommended so that your scripts(if there are any) can interact with the cluster services which may not be publicly exposed.\nIf you want to run it inside your application then you have to check the Execute in application Environment option else leave it unchecked.\n(** Make sure your cluster has devtron-agent installed if you Check the Execute in the application Environment option.)\n    4. Deployment Stages We support two types of deployments- Manual and Automatic. If you select automatic, it will trigger your CD pipeline automatically once your corresponding CI pipeline has built successfully.\nIf you have defined pre-deployment stages, then CD Pipeline will be triggered automatically after the successful build of your CI pipeline followed by the successful build of your pre-deployment stages. But if you select manual, then you have to trigger your deployment via console.\n5. Deployment Strategy Devtron\u0026rsquo;s tool has 4 types of deployment strategies. Click on Add Deployment strategy and select from the available options. Options are:\n(a) Recreate\n(b) Canary\n(c) Blue Green\n(d) Rolling\n6. Post-deployment Stages If you want to Configure actions like Jira ticket close, that you want to run after the deployment, you can configure such actions in post-deployment stages.\nPost-deployment stages are similar to pre-deployment stages. The difference is, pre-deployment executes before the CD pipeline execution and post-deployment executes after the CD pipeline execution. The configuration of post-deployment stages is similar to the pre-deployment stages.\nYou can use Config Map and Secrets in post deployments as well, as defined in Pre-Deployment stages\n    You have configured the CD pipeline, now click on Create Pipeline to save it. You can see your newly created CD Pipeline on the Workflow tab attached to the corresponding CI Pipeline.\n    The CD Pipeline is created\nUpdate CD Pipeline You can update the CD Pipeline. Updates such as- adding Deployment Stage, Deployment Strategy. But you cannot update the name of a CD Pipeline or it’s Deploy Environment. If you need to change such configurations, you need to make another CD Pipeline.\nTo Update a CD Pipeline, go to the App Configurations and then click on Workflow editor and click on your CD Pipeline you want to Update.\n       Make changes as per your requirement and click on Update Pipeline to update this CD Pipeline.\nDelete CD Pipeline If you no longer require the CD Pipeline, you can also Delete the Pipeline.\nTo Delete a CD Pipeline, go to the App Configurations and then click on the Workflow editor. Now Click on the pipeline you want to delete. A pop will be displayed with CD details. Now click on the Delete Pipeline option to delete this CD Pipeline\n    Deployment Strategies A deployment strategy is a way to make changes to an application, without downtime in a way that the user barely notices the changes. There are different types of deployment strategies like Blue/green Strategy, Rolling Strategy, Canary Strategy, Recreate Strategy. These deployment configuration-based strategies are discussed in this section.\nBlue Green Stategy Blue-green deployments involve running two versions of an application at the same time and moving traffic from the in-production version (the green version) to the newer version (the blue version).\nblueGreen: autoPromotionSeconds: 30 scaleDownDelaySeconds: 30 previewReplicaCount: 1 autoPromotionEnabled: false    Key Description     autoPromotionSeconds It will make the rollout automatically promote the new ReplicaSet to active Service after this time has passed   scaleDownDelaySeconds It is used to delay scaling down the old ReplicaSet after the active Service is switched to the new ReplicaSet.   previewReplicaCount It will indicate the number of replicas that the new version of an application should run   autoPromotionEnabled It will make the rollout automatically promote the new ReplicaSet to the active service.    Rolling Strategy A rolling deployment slowly replaces instances of the previous version of an application with instances of the new version of the application. Rolling deployment typically waits for new pods to become ready via a readiness check before scaling down the old components. If a significant issue occurs, the rolling deployment can be aborted.\nrolling: maxSurge: \u0026#34;25%\u0026#34; maxUnavailable: 1    Key Description     maxSurge No. of replicas allowed above the scheduled qauntity.   maxUnavailable Maximum number of pods allowed to be unavailable.    Canary Strategy Canary deployments are a pattern for rolling out releases to a subset of users or servers. The idea is to first deploy the change to a small subset of servers, test it, and then roll the change out to the rest of the servers. The canary deployment serves as an early warning indicator with less impact on downtime: if the canary deployment fails, the rest of the servers aren\u0026rsquo;t impacted.\ncanary: maxSurge: \u0026#34;25%\u0026#34; maxUnavailable: 1 steps: - setWeight: 25 - pause: duration: 15 # 1 min - setWeight: 50 - pause: duration: 15 # 1 min - setWeight: 75 - pause: duration: 15 # 1 min    Key Description     maxSurge It defines the maximum number of replicas the rollout can create to move to the correct ratio set by the last setWeight   maxUnavailable The maximum number of pods that can be unavailable during the update   setWeight It is the required percent of pods to move to next step   duration It is used to set the duration to wait to move to the next step.    Recreate The recreate strategy is a dummy deployment which consists of shutting down version A then deploying version B after version A is turned off. A recreate deployment incurs downtime because, for a brief period, no instances of your application are running. However, your old code and new code do not run at the same time.\nrecreate: It terminate the old version and release the new one.\nDoes your app has different requirements in different Environments? Also read Environment Overrides\n"
},
{
	"uri": "/cloning_applications/",
	"title": "Cloning Application",
	"tags": null,
	"description": "",
	"content": "   Select Add New App to Create a new app.\n       Key Description     App Name Name of the new app you want to Create   Project Project name   Template Select the App whose template you want to use to the Create new app    Click on Duplicate App to create App with a template of the Application you have selected from the Drop-down.\n    New application with a duplicate template is created.\n"
},
{
	"uri": "/use_cases/connect_spring_boot_with_mysql_database/",
	"title": "Connect SpringBoot with Mysql Database",
	"tags": null,
	"description": "",
	"content": "Introduction This document will help you to deploy a sample Spring Boot Application, using mysql Helm Chart\n1. Deploy a mysql Helm Chart To deploy mysql Helm Chart, you can refer to our documentation on Deploy mysql Helm Chart\n2. Fork the Git Repository For this example, we are using the following GitHub Repo, you can clone this repository and make following changes in the files.\n* Configure application.properties Set the database configuration in this file.\nspring.datasource.url=jdbc:mysql://\u0026lt;service-name\u0026gt;/\u0026lt;mysql database-name\u0026gt; spring.jpa.hibernate.ddl-auto=update spring.jpa.show-sql=true spring.datasource.username=\u0026lt;mysql-user\u0026gt; spring.datasource.password=\u0026lt;mysql-password\u0026gt; spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5Dialect spring.jpa.open-in-view=true * Configure the Dockerfile # syntax=docker/dockerfile:experimental FROM maven:3.5-jdk-8-alpine as build WORKDIR /workspace/app COPY pom.xml . RUN mvn -B -e -C -T 1C org.apache.maven.plugins:maven-dependency-plugin:3.0.2:go-offline COPY . . RUN mvn clean package -Dmaven.test.skip=true FROM openjdk:8-jdk-alpine RUN addgroup -S demo \u0026amp;\u0026amp; adduser -S demo -G demo VOLUME /tmp USER demo ARG DEPENDENCY=/workspace/app/target/dependency COPY --from=build /workspace/app/target/docker-demo-0.0.1-SNAPSHOT.jar app.jar ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-jar\u0026#34;, \u0026#34;app.jar\u0026#34;]  3. Create Application on Devtron To learn how to create an application on Devtron, refer to our documentation on Creating Application\n* Git Material In this example, we are using the url of the forked Git Repository.\n* Docker configuration Give, the path of the Dockerfile.\n* Configure Deployment Template Enable Ingress, and give the path on which you want to host the application.\n  * Set up the CI/CD Pipelines Set up the CI/CD pipelines. You can set them to trigger automatically or manually.\n* Trigger Pipelines Trigger the CI Pipeline, build should be Successful. Then trigger the CD Pipeline, deployment pipeline will be initiated, after some time the status should be Healthy.\n4. Final Step * Test Rest API It exposes 3 REST endpoints for it\u0026rsquo;s users to create, to view specific student record and view all student records.\nTo test Rest API, you can use curl command line tool\nCreate a new Student Record\nCreate a new POST request to create a new Transaction. Once the transaction is successfully created, you will get the student id as a response.\nCurl Request is as follows:\nsudo curl -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Anushka\u0026#34;, \u0026#34;marks\u0026#34;: 98}\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; -X POST http://\u0026lt;hostname\u0026gt;/\u0026lt;path-name\u0026gt;/create View All Student\u0026rsquo;s Data\nTo view all student records, GET Request is:\n path will be the one that you have given in Step 3 while configuring the Deployment Template.\nhttp://\u0026lt;hostname\u0026gt;/\u0026lt;path\u0026gt;/viewAll\n    View student\u0026rsquo;s data By student ID\nTo view student data by student id, GET Request is:\nhttp://\u0026lt;hostname\u0026gt;/\u0026lt;path\u0026gt;/view/\u0026lt;id\u0026gt;\npath will be the one that you have given in Step 3 while configuring the Deployment Template.\n   "
},
{
	"uri": "/creating_application/docker_configuration/",
	"title": "Docker Build Configuration",
	"tags": null,
	"description": "",
	"content": "In the previous step, we discussed git configurations. In this section, we will provide information on the docker build config.\nDocker build configuration is used to create and push docker images in the docker registry for your application. You will provide docker related information to build and push docker images in this step.\nOnly one docker image can be created even for multi-git repository application as explained in the previous step.\n  Here you can see, 5 options are present to configure your docker build.\n  Repository\n  Docker file path\n  Docker Registry\n  Docker Repository\n  Docker build arguments\n  Key\n  Value\n       Options Description     Repository Provide the checkout path of the repository in this column, which you had defined earlier in git configuration details   Docker File Path Provide a relative path for your docker file. Dockerfile should be present on this path.    |Docker Registry|Select the docker registry you wish to use, which will be used to store docker images.| |Docker Repository|Name of your docker repository that will store a collection of related images. Every image is stored with a new tag version.| |Key-value|The key parameter and the value for a given key for your docker build. This is Optional. (this can be overridden at CI step later)|\n1. Repository Provide the checkout path of the repository in this column, which you defined earlier in git configuration details. In the case of multi-git, select the checkout path configured for the repository containing Dockerfile to be used for image creation.\n2. Docker file path Inside the Docker file path option, provide a relative path for your docker file. The default docker file name is Dockerfile but if you are using a custom name for your Dockerfile like xyz_dockerfile, then you have to provide that custom name.\nIf your docker file is not at the root of the repository, then it should contain a relative path to this file. The path will be searched inside the checkout path selected in the previous step.\n3. Docker Registry Select the docker registry you want to use to store docker images. You can have ECR (Elastic Container Registry), DockerHub, etc, and many other registries as your docker registry.\nAdding a registry in the drop-down is configurable. To get a drop-down of these registries into the docker registry option, you have to add the configuration and credentials in the Global Configuration.\n4. Docker Repository You have to provide the docker Registry(ECR) bucket name under the Docker Repository option, but this is optional. If you provide the name of your repository we will use that name but if you don’t provide a name then the docker repository is automatically created and used.\n5. Docker build arguments Many times you use some arguments when you build your Dockerfile. Here you can provide those arguments in key-value format. You can provide one or more arguments and these arguments are optional. Read about the docker command line and arguments.\nOnce all the configuration is done, click on Save Configuration to save the Docker Build Configuration.\n"
},
{
	"uri": "/global_configurations/docker_registries/",
	"title": "Docker Registries",
	"tags": null,
	"description": "",
	"content": "Add Docker Registry configuration: Go to the Docker Registry section of Global Configuration. Click on Add docker registry.\nYou will see below the input fields to configure the docker registry.\n  Name\n  Registry type\n  Ecr\n  AWS region\n  Access key ID\n  Secret access key\n    Others\n  Username\n  password\n      Registry URL\n  Set as default\n      Name Provide a name to your registry, this name will be shown to you in Docker Build Config as a drop-down.\nRegistry type Here you can select the type of the Registry. We are supporting two types- ecr and others. You can select any one of them from the drop-down. By default, this value is ecr. If you select ecr then you have to provide some information like- AWS region, Access Key, and Secret Key. And if you select others then you have to provide the Username and Password.\nRegistry URL Select any type of Registry from the drop-down, you have to provide the URL of your registry. Create your registry and provide the URL of that registry in the URL box.\nRegistry Type- ECR: You have to provide the below information if you select the registry type as ECR.\n AWS region  Select your AWS region from the drop-down, region where you have created your registry in.\n Access key ID  Inside the Access key ID box, provide your AWS access key.\n Secret access key  Provide your AWS secret access key ID.\n    Registry Type Others: You have to provide the below information if you select the registry type as others.\n Username  Give the username of your account, where you have created your registry in.\n Password  Give the password corresponding to the username of your registry.\n    Set as default: If you enable the Set as default option, then this registry name will be set as default in the Docker Registry section inside the Docker build config page. This is optional. You can keep it disabled.\nNow click on Save to save the configuration of the Docker registry.\n"
},
{
	"uri": "/deploy_chart/examples/",
	"title": "Examples",
	"tags": null,
	"description": "",
	"content": "This Documentation guides you to Deploy different Helm Charts available on Devtron.\nParts of Documentation\nDeploying mySQL Helm Chart Deploying mongoDB Helm Chart\n"
},
{
	"uri": "/deploying_applications/triggering_cd/",
	"title": "Triggering CD",
	"tags": null,
	"description": "",
	"content": "Triggering CD Pipelines     After CI Pipeline is complete, CD Pipeline can be triggered by Clicking on Select Image.\n    Select an image to deploy and then Click on Deploy to trigger the CD Pipeline.\nThe running images are tagged as Running\n    The status of the current deployment can be viewed by Clicking on App Details that will show the Progressing state for 1-2 minutes and then gradually shows Healthy state or Hibernating state, based on the deployment strategy.\nHere, triggering CD Pipeline is successful and the deployment is in \u0026ldquo;Healthy\u0026rdquo; state.\nTo further diagnose deployments, Click here\n"
},
{
	"uri": "/global_configurations/cluster_and_environment/",
	"title": "Cluster And Environments",
	"tags": null,
	"description": "",
	"content": "The Global configuration provides a feature of Cluster \u0026amp; Environment in which you can add your Kubernetes clusters and environment.\nSelect the Cluster \u0026amp; Environment section of global configuration and click on Add Cluster to add your cluster.\nAdd Cluster: Provide the below information to add your kubernetes cluster:\n  Name\n  Kubernetes Cluster Info\n  Server URL\n  Bearer token\n    Prometheus Info\n  Prometheus endpoint\n  Basic\n  Username\n  Password\n    Anonymous\n    TLS Key\n  TLS Certificate\n        1. Name Give a name to your cluster inside the name box.\n2. Kubernetes Cluster Info Provide your kubernetes cluster’s credentials.\n Server URL  Provide the endpoint/URL of your kubernetes cluster.\n Bearer token  Provide your kubernetes cluster’s Bearer token for authentication purposes so that the Devtron tool will be able to talk to your kubernetes cluster and can deploy your application in your kubernetes cluster.\n3. Prometheus Info Prometheus is a powerful solution to provide graphical insight into your application behavior. If you want to see your application matrix against your applications deployed in kubernetes, install Prometheus in your kubernetes cluster. The below inputs are required to configure your prometheus into Devtron’s tool.\n Prometheus endpoint  Provide the URL of your prometheus. Prometheus supports two types of authentication Basic and Anonymous. Select the authentication type for your Prometheus setup.\n Basic  If you select the basic type of authentication then you have to provide the Username and Password of prometheus for authentication.\n Anonymous  If you select Anonymous then you do not have to provide any username and password for authentication.\n TLS Key \u0026amp; TLS Certificate  TLS key and TLS certificate both options are optional, these options are used when you use a custom URL, in that case, you can pass your TLS key and TLS certificate.\nCheck the below screenshots to know how it looks like If you select the Basic authentication type\n    If you select the Anonymous authentication type\n    Now click on Save Cluster to save your cluster information.\nNote: Your kubernetes cluster gets mapped with the Devtron when you save your kubernetes cluster Configuration. Now the agents of devtron will be installed on your cluster so that the components of devtron can communicate to your cluster. When the agent starts installing on your cluster, you can check the status of the agents in the Cluster \u0026amp; Environment tab also.\n    Click on Details to check what got installed inside the agents. A new window will be popped up displaying all the details about these agents.\n    Add Environment Once you have added your cluster in Cluster \u0026amp; Environment, you can add the environment also. Click on Add Environment, a window will be opened. Give a name to your environment in the Environment Name box and provide a namespace corresponding to your environment in the Namespace input box. Now choose if your environment is for Production purposes or for Non-production purposes. Production and Non-production options are only for tagging purposes. Click on Save and your environment will be created.\n    Update Environment You can update an already created environment, Select and click on the environment which you want to update. You can only change Production and Non-production options here.\nNote- you can not change the Environment name and Namespace name.\n    "
},
{
	"uri": "/use_cases/connect_expressjs_with_mongodb_database/",
	"title": "Connect Expressjs With Mongodb Database",
	"tags": null,
	"description": "",
	"content": "Introduction In this application, you will learn about how to create a Expressjs Application that connects to mongoDb.\nFollow the below mentioned steps, to deploy the application on Devtron using mongoDb Helm Chart.\n1. Deploy mongoDb Helm Chart To deploy mongoDb Helm Chart, you can refer to our documentation on Deploy mongoDb Helm Chart\n2. Fork the Git Repository For this example, we are using the following GitHub Repo, you can clone this repository and make following changes in the files.\n* Dockerfile This is the Dockerfile. This exposes our expressjs application to port number 8080\n FROM node:7 WORKDIR /app COPY package.json /app RUN npm install COPY . /app CMD node server.js EXPOSE 8080 * db.js File This file will be used to connect to our database. This will include the service-name of the mongoDb Helm Chart, that you have deployed in Step1.\nThe syntax is as follows:\n\u0026lt;service-name\u0026gt;:27017/\u0026lt;database-name\u0026gt;\nThis maps our service name to mongoDb\u0026rsquo;s port number 27017.\n module.exports = { DB: \u0026#39;mondo-dev-mongodb-replicaset-client:27017/sale\u0026#39; }  3. Create Application on Devtron To learn how to create an application on Devtron, refer to our documentation on Creating Application\n* Git Material In this example, we are using the url of the forked Git Repository.\n* Docker configuration Give, the path of the Dockerfile.\n* Configure Deployment Template Enable Ingress, and give the path on which you want to host the application.\n  * Set up the CI/CD Pipelines Set up the CI/CD pipelines. You can set them to trigger automatically or manually.\n* Trigger Pipelines Trigger the CI Pipeline, build should be Successful, then trigger the CD Pipeline, deployment pipeline will be initiated, after some time the status should be Healthy\n4. Final Step Check the Expressjs app connected to mongodb database, running successfully by hitting your application url.\nThe syntax is: http://\u0026lt;hostname\u0026gt;/\u0026lt;path\u0026gt;/\npath will be the one that you have given in Step 3 while configuring the Deployment Template.\nThe output of our application would be as follows:\n  You can see that we are getting the JSON response. We have successfully connected our expressjs application to the mongoDb database.\n"
},
{
	"uri": "/deploying_applications/",
	"title": "Deploying Application",
	"tags": null,
	"description": "",
	"content": "Welcome! This is the documentation for Deploying Applications\nParts of Documentation\nTriggering CI Triggering CD\n"
},
{
	"uri": "/deploy_chart/examples/deploying_mysql_helm_chart/",
	"title": "Deploying Mysql Helm Chart",
	"tags": null,
	"description": "",
	"content": "Introduction stable/mysql Helm chart bootstraps a single node MySQL deployment on a Kubernetes cluster using the Helm package manager.\n    1. Discover the chart from the Chart Store Select the Charts section from the left pane, you will be landed to the Chart Store page. Click on Discover and find stable/mongodb-replicaset Helm Chart.\n    2. Configure the Chart After selecting the stable/mySQL Helm chart, click on Deploy\nEnter the following details, to deploy mysql chart:\n   Key Description     App Name Name of the Chart   Project Select the name of your Project in which you want to deploy the chart   Environment Select the environment in which you want to deploy the chart   Chart Version Select the latest Chart Version   Chart Value Select the latest default value or create a custom value     Configure values.yaml Set the following parameters in the chart, to be later used to connect mysql with your Django Application.\n       Parameters Description     mysqlRootPassword Password for the root user. Ignored if existing secret is provided   mysqlDatabase Name of your mysql database   mysqluser Username of new user to create   mysqlPassword Password for the new user. Ignored if existing secret is provided        Click on Deploy to deploy the Chart\n3. Check the Status of Deployment After clicking on Deploy you will land on a page, that shows the Status of the deployment of the Chart.\nThe Status of the chart should be Healthy. It might take few seconds after initiating the deployment of the chart.\n    In case the Status, of the deployment is Degraded or takes a long time to get deployed.\nClick on the Status or check the logs of the pods to debug the issue.\n4. Extract the Service Name Copy the service name, it will be used to connect your application to mySQL.\n    "
},
{
	"uri": "/creating_application/deployment_template/",
	"title": "Deployment Template",
	"tags": null,
	"description": "",
	"content": "Deployment configuration is the Manifest for the application, it defines the runtime behavior of the application. You can define application behavior by providing information in three sections:\n  Chart Version\n  Yaml file\n  Show application metrics\n    1. Chart version    Key Descriptions     Chart Version Select the Chart Version using which you want to deploy the application.    Devtron uses helm charts for the deployments. And we are having multiple chart versions based on features it is supporting with every chart version.\nOne can see multiple chart version options available in the drop-down. you can select any chart version as per your requirements. By default, the latest version of the helm chart is selected in the chart version option.\nEvery chart version has its own YAML file. Helm charts are used to provide specifications for your application. To make it easy to use, we have created templates for the YAML file and have added some variables inside the YAML. You can provide or change the values of these variables as per your requirement.\nIf you want to see Application Metrics (For example Status codes 2xx, 3xx, 5xx; throughput, and latency) for your application, then you need to select the latest chart version.\nApplication Metrics is not supported for Chart version older than 3.7 version.\n2. Yaml file Container This defines ports on which application services will be exposed to other services\nContainerPort: name: app port: 8080 servicePort: 80    Key Description     name name of the container   port port for the container   servicePort service port for the container    Liveness Probe If this check fails, kubernetes restarts the pod. This should return error code in case of non-recoverable error\nLivenessProbe: Path: \u0026#34;\u0026#34; port: 8080 initialDelaySeconds: 20 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 failureThreshold: 3     Key Description     Path It define the path where the liveness needs to be checked.   failureThreshold It defines the maximum number of failures that are acceptable before a given container is not considered as live.   initialDelaySeconds It defines the time to wait before a given container is checked for liveliness.   periodSeconds It defines the time to check a given container for liveness.   successThreshold It defines the number of successes required before a given container is said to fulfil the liveness probe.   timeoutSeconds It defines the time for checking timeout.    Readiness Probe If this check fails, kubernetes stops sending traffic to the application. This should return error code in case of errors which can be recovered from if traffic is stopped\nReadinessProbe: Path: \u0026#34;\u0026#34; port: 8080 initialDelaySeconds: 20 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 failureThreshold: 3    Key Description     Path It define the path where the rediness needs to be checked.   failureThreshold It defines the maximum number of failures that are acceptable before a given container is not considered as ready.   initialDelaySeconds It defines the time to wait before a given container is checked for readiness.   periodSeconds It defines the time to check a given container for readiness.   successThreshold It defines the number of successes required before a given container is said to fulfil the rediness probe.   timeoutSeconds It defines the time for checking timeout.    Autoscaling This is connected to HPA and controls scaling up and down in response to request load\nautoscaling: enabled: false MinReplicas: 1 MaxReplicas: 2 TargetCPUUtilizationPercentage: 90 TargetMemoryUtilizationPercentage: 80    Key Description     MaxReplicas Maximum number of replicas allowed for scaling.   MinReplicas Minimum number of replicas allowed for scaling.   TargetCPUUtilizationPercentage The target CPU utilization that is expected for a container.   TargetMemoryUtilizationPercentage The target memory utilization that is expected for a container.   enabled to enable autoscaling or don\u0026rsquo;t enable it.    Image image: pullPolicy: IfNotPresent Image is used to access images in kubernetes, pullpolicy is used to define the instances calling the image, here the image is pulled when the image is not present,it can also be set as \u0026ldquo;Always\u0026rdquo;.\nIngress This allows public access to the url, please ensure you are using right nginx annotation for nginx class, its default value is nginx\ningress: enabled: false annotations: {} path: \u0026#34;\u0026#34; host: \u0026#34;\u0026#34; tls: []    Key Description     enabled Enable or disable ingress   annotations To configure some options depending on the Ingress controller   path Path name   host Host name   tls It contains security details    Ingress Internal This allows private access to the url, please ensure you are using right nginx annotation for nginx class, its default value is nginx\ningressInternal: enabled: false annotations: {} path: \u0026#34;\u0026#34; host: \u0026#34;\u0026#34; tls: []    Key Description     enabled Enable or disable ingress   annotations To configure some options depending on the Ingress controller   path Path name   host Host name   tls It contains security details    Resources These define minimum and maximum RAM and CPU available to the application\nresources: limits: cpu: \u0026#39;1\u0026#39; memory: 200Mi requests: cpu: \u0026#39;0.10\u0026#39; memory: 100Mi Resources are required to set CPU and memory usage.\nLimits Limits make sure a container never goes above a certain value. The container is only allowed to go up to the limit, and then it is restricted.Requests Requests are what the container is guaranteed to get.\nService This defines annotations and the type of service, optionally can define name also\nservice: type: ClusterIP annotations: {} Volumes volumes: [] It is required when some values need to be read from or written to an external disk.\nVolume Mounts volumeMounts: [] It is used to provide mounts to the volume\nAffinity and anti-affinity Spec: Affinity: Key: Values: Spec is used to define the desire state of the given container.\nNode Affimity allows you to constrain which nodes your pod is eligible to schedule on, based on labels of the node.\nInter-pod affinity allow you to constrain which nodes your pod is eligible to be scheduled based on labels on pods.\nKey Key part of the label for node selection, this should be same as that on node. Please confirm with devops team\nValues Value part of the label for node selection, this should be same as that on node. Please confirm with devops team\nTolerations tolerations: key: \u0026#34;key\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value\u0026#34; effect: \u0026#34;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026#34; Taints are the opposite, they allow a node to repel a set of pods.\nA given pod can access the given node and avoid the given taint only if the given pod satisfies a given taint.\nTaints and tolerations work together to ensure that pods are not scheduled onto the inappropriate nodes. One or more taints can be applied to a node, this marks that the node should not accept any pods that don\u0026rsquo;t tolerate the taints.\nArguments args: enabled: false value: [] This is used to give arguments to command\nCommand command: enabled: false value: [] It contains the commands for the server.\n   Key Description     enabled To enable or disable the command.   value It contains the commands.    Prometheus prometheus: release: monitoring It is a kubernetes monitoring tool and the name of the file to be monitored as monitoring in the given case.It describes the state of the prometheus.\nGrace Period GracePeriod: 30 If it has expired then the task is requeued to be executed again.\nMin Ready Seconds MinReadySeconds: 60 Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available\nServer server: deployment: image_tag: 1-95a53 image: \u0026#34;\u0026#34; It is used for providing server configurations.\nDeployment It gives the details for deployment\n   Key Description     image_tag It is the image tag   image It is the URL of the image    Service Monitor servicemonitor: enabled: true path: /abc scheme: \u0026#39;http\u0026#39; interval: 30s scrapeTimeout: 20s metricRelabelings: - sourceLabels: [namespace] regex: \u0026#39;(.*)\u0026#39; replacement: myapp targetLabel: target_namespace It gives the set of targets to be monitored.\nDb Migration Config dbMigrationConfig: enabled: false It is used to configure database migration\nApplication Metrics Application metrics can be enabled to see your application\u0026rsquo;s metrics-CPUService Monito usage,Memory Usage,Status,Throughput and Latency.\nDeployment Metrics A deployment strategy is a way to make changes to your application, without downtime in a way that your application user barely notices the changes. There are different types of deployment strategies. To know about Deployment Strategies, Click on: Types of Deployment Strategies\nAdd on features in Deployment Chart version 3.9.0 Service Account serviceAccountName: orchestrator A service account provides an identity for the processes that run in a Pod.\nWhen you access the cluster, you are authenticated by the apiserver as a particular User Account. Processes in containers inside pod can also contact the apiserver. When you are authenticated as a particular Service Account.\nWhen you create a pod, if you do not create a service account, it is automatically assigned the default service account in the namespace.\nPod Disruption Budget podDisruptionBudget: {} minAvailable: 1 maxUnavailable: 1 You can create PodDisruptionBudget for each application. A PDB limits the number of pods of a replicated application that are down simultaneously from voluntary disruptions. For example, an application would like to ensure the number of replicas running is never brought below the certain number.\nYou can specify maxUnavailable and minAvailable in a PodDisruptionBudget.\nWith minAvailable of 1, evictions are allowed as long as they leave behind 1 or more healthy pods of the total number of desired replicas.\nWith maxAvailable of 1, evictions are allowed as long as atmost 1 unhealthy replica among the total number of desired replicas.\nApplication metrics Envoy Configurations envoyproxy: image: envoyproxy/envoy:v1.14.1 configMapName: \u0026#34;\u0026#34; resources: limits: cpu: 50m memory: 50Mi requests: cpu: 50m memory: 50Mi Envoy is attached as a sidecar to the application container to collect metrics like 4XX, 5XX, Throughput and latency. You can now configure the envoy settings such as idleTimeout, resources etc.\nPrometheus Rule prometheusRule: enabled: true additionalLabels: {} namespace: \u0026#34;\u0026#34; rules: - alert: TooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\u0026#34;5.+\u0026#34;} ) / sum(nginx_ingress_controller_requests) ) \u0026gt; 5 for: 1m labels: severity: critical annotations: description: Too many 5XXs summary: More than 5% of the all requests did return 5XX, this require your attention Alerting rules allow you to define alert conditions based on Prometheus expressions and to send notifications about firing alerts to an external service.\nIn this case, Prometheus will check that the alert continues to be active during each evaluation for 1 minute before firing the alert. Elements that are active, but not firing yet, are in the pending state.\nCustom Metrics in HPA autoscaling: enabled: true MinReplicas: 1 MaxReplicas: 2 TargetCPUUtilizationPercentage: 90 TargetMemoryUtilizationPercentage: 80 HPA will be able to give metrics such as CPU and memory usage for cluster nodes or any of the pods. These metrics are useful for internal cluster sizing, but you probably want to configure wider set of metrics like service latency, I/O load etc.\nThe custom metrics in HPA can help you to achieve this.\n 3. Show application metrics  If you want to see application matrics like different status codes, application throughput, latency, response time. Enable the Show Application matrix from here. And you will get all metrics on App detail page. This is optional. You can leave it disabled. By default it remains disabled.\n    Once all the Deployment template configurations are done, click on Save to save your deployment configuration. Now you are ready to create Workflow to do CI/CD.\n"
},
{
	"uri": "/use_cases/connect_django_with_mysql_database/",
	"title": "Connect Django With Mysql Database",
	"tags": null,
	"description": "",
	"content": "Introduction Django is a free, open-source web framework written in Python programming language. It allows for scalability, re-usability, and rapid development. Django can be connected to different databases like MySQL, PostgreSQL, etc.\n1. Deploy a mysql Helm Chart To deploy mysql Helm Chart, you can refer to our documentation on Deploy mysql Helm Chart\n2. Fork the Git Repository For this example, we are using the following GitHub Repo, you can clone this repository and make following changes in the files.\n* Configure Database Settings Go to mysite/settings.py.\nThe settings.py contains the configuration for your SQL database. Make sure the configurations in settings.py matches the configurations of the mysql Helm Chart, that you have deployed in Step 1.\nDATABASES = { \u0026#39;default\u0026#39;: { # If you are using Cloud SQL for MySQL rather than PostgreSQL, set # \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.mysql\u0026#39; instead of the following. \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.postgresql\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;\u0026lt;mysql-database\u0026gt;\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;\u0026lt;mysql-user\u0026gt;\u0026#39;, \u0026#39;PASSWORD\u0026#39;: \u0026#39;\u0026lt;mysql-password\u0026gt;\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;\u0026lt;service-name\u0026gt;\u0026#39;, \u0026#39;PORT\u0026#39;: \u0026#39;3306\u0026#39;, } }  3. Create Application on Devtron To learn how to create an application on Devtron, refer to our documentation on Creating Application\n* Git Material In this example, we are using the url of the forked Git Repository.\n* Docker configuration Give, the path of the Dockerfile.\n* Configure Deployment Template Enable Ingress, and give the path on which you want to host the application.\n  * Set up the CI/CD Pipelines Set up the CI/CD pipelines. You can set them to trigger automatically or manually.\n* Trigger Pipelines Trigger the CI Pipeline, build should be Successful, then trigger the CD Pipeline, deployment pipeline will be initiated, after some time the status should be Healthy.\n4. Final Step Check the Django app connected to mysql database, running successfully by hitting your application url.\nThe syntax is: http://\u0026lt;hostname\u0026gt;/\u0026lt;path\u0026gt;/\npath will be the one that you have given in Step 3 while configuring the Deployment Template.\n   "
},
{
	"uri": "/deploy_chart/",
	"title": "Deploy Chart",
	"tags": null,
	"description": "",
	"content": "Welcome! This is the documentation for Deploying Charts.\nParts of Documentation\nOverview of Charts Examples\n"
},
{
	"uri": "/deploy_chart/examples/deploying_mongodb_helm_chart/",
	"title": "Deploying MongoDB Helm Chart",
	"tags": null,
	"description": "",
	"content": "Introduction Let\u0026rsquo;s assume that you are building an application which needs mongoDB.\n   Deploying applications as Helm Charts is the easiest way to create applications on Devtron.\nThis guide will introduce you to how to deploy the mongoDB\u0026rsquo;s Helm chart.\n1. Discover the Chart from the Chart Store Select the Charts section from the left pane, you will be landed to the Chart Store page. Click on Discover and find stable/mongodb-replicaset Helm Chart.\n    2. Configure the Chart After selecting the stable/mongodb helm chart, click on Deploy\n    Enter the following details, to deploy mongoDB chart:\n   Key Description     App Name Name of the Chart   Project Select the name of your Project in which you want to deploy the chart   Environment Select the environment in which you want to deploy the chart   Chart Version Select the latest Chart Version   Chart Value Select the latest default value or create a custom value    Configure values.yaml In this example replicas is set to 1 and persistenceVolume is set to false. You can configure it according to your project\u0026rsquo;s requirements.\nTo learn about different parameters used in the chart, you can check Documentation of mongodb Helm chart\n    Click on Deploy after you have finished configuring the chart.\n3. Check the Status of Deployment After clicking on Deploy you will land on a page, that shows the Status of the deployment of the Chart.\nThe Status of the chart should be Healthy. It might take few seconds after initiating the deployment of the chart.\n    In case the Status of the deployment is Degraded or takes a long time to get deployed.\nClick on the Status or check the logs of the pods to debug the issue.\n4. Extract the Service name Copy the service name, it will be used to connect your application to mongoDB .\n    "
},
{
	"uri": "/global_configurations/projects/",
	"title": "Projects",
	"tags": null,
	"description": "",
	"content": "Projects are nothing but a logical grouping of your applications so that you can manage and control the access level of users. We will discuss User Access in the next step.\nAdd Project: Click on the Projects inside the Global configuration tab. Click on Add projects and give a name to your project and press the Save button to save your project\n    "
},
{
	"uri": "/creating_application/workflows/",
	"title": "Workflow",
	"tags": null,
	"description": "",
	"content": "Workflow is a logical sequenece of different stages used for continous integration and continous deployment of an application.\n  Click on Create Workflow to create a new workflow\n  Enter the name of your workflow and then Click on Add Workflow to add a new workflow. Workflow always starts from Git Material.\nThen, Create CI/CD Pipelines for your application.\nTo know how to Create the CI Pipeline for your application Click On: Create CI Pipelines\nTo know how to Create the CD Pipeline for your application Click On: Create CD Pipelines\n "
},
{
	"uri": "/debugging_deployments_and_monitoring/",
	"title": " Debugging Deployment And Monitoring",
	"tags": null,
	"description": "",
	"content": "Debugging Deployments If the deployment your application is not successful, then debugging needs to be done to check the cause of the error.\nThis can be done through App Details section which you can access in the following way:-\nApplications-\u0026gt;AppName-\u0026gt;App Details\nOver here, you can see the status of the app as Healthy. If there are some errors with deployment then the status would not be in a Healthy state.\nEvents     Events of the application are accessible from the bottom left corner.\nEvents section displays you the events that took place during the deployment of an app. These events are available until 15 minutes of deployment of the application.\nLogs     Logs contain the logs of the Pods and Containers deployed which you can use for the process of debugging.\nManifest     The Manifest shows the critical information such as Container-image, restartCount, state, phase, podIP, startTime etc. and status of the pods deployed.\nDeleting Pods     You might run into a situation where you need to delete Pods. You may need to bounce or restart a pod.\nDeleting a Pod is not an irksome task, it can simply be deleted by Clicking on Delete Pod.\nSuppose you want to setup a new environment, you can delete a pod and thereafter a new pod will be created automatically depending upon the replica count.\nApplication Objects You can view Application Objects in this section of App Details, such as:\n   Key Description     Workloads ReplicaSet(ensures how many replica of pod should be running), Status of Pod(status of the Pod)   Networking Service(an abstraction which defines a logical set of Pods), Endpoints(names of the endpoints that implement a Service), Ingress(API object that manages external access to the services in a cluster)   Config \u0026amp; Storage ConfigMap( API object used to store non-confidential data in key-value pairs)   Custom Resource Rollout(new Pods will be scheduled on Nodes with available resources), ServiceMonitor(specifies how groups of services should be monitored)        Monitoring     You can monitor the application in the App Detailssection.\nMetrics like CPU Usage, Memory Usage, Throughput and Latency can be viewed here.\n   Key Description     CPU Usage Percentage of CPU\u0026rsquo;s cycles used by the app.   Memory Usage Amount of memory used by app.   Throughput Performance of the app.   Latency Delay caused while transmitting the data.    "
},
{
	"uri": "/creating_application/config_maps/",
	"title": "Config Maps",
	"tags": null,
	"description": "",
	"content": "The ConfigMap API resource holds key-value pairs of configuration data that can be consumed in pods or used to store configuration data for system components such as controllers. ConfigMap is similar to Secrets, but designed to more conveniently support working with strings that do not contain sensitive information.\nClick on Add ConfigMap to add a config map to your application.\n    Configure the ConfigMap You can configure the configmap in two way-\n(a) Using data type Kubernetes ConfigMap\n(b) Using data type Kubernetes External ConfigMap\n(A) Using Kubernetes ConfigMap    Key Description     Data Type(Kubernetes ConfigMap) Select data type from Kubernetes ConfigMap and Kubernetes External ConfigMap   Name Provide a name to this ConfigMap.   Use configmap as Environment Variable Select this option if you want to inject Environment Variables in pods using ConfigMap.   Use configmap as Data Volume Select this option, if you want to configure any Data Volume that is accessible to Containers running in a pod and provide a Volume mount path.   Key-Value Provide the actual key-value configuration data here. Key and corresponding value to the provided key.        1. Data Type Select the Data Type as Kubernetes ConfigMap, if you wish to use the ConfigMap created by Devtron. You can select Kubernetes External ConfigMap if you have created a ConfigMap using the Kubectl command.\nBy default, the data type is selected to Kubernetes ConfigMap.\n2. Name Provide a name to your configmap.\n3. Use ConfigMap as Here we are providing two options, one can select any of them as per the requirement\n-Environment Variable as part of your configMap or you want to add Data Volume to your container using Config Map.\n Environment Variable  Select this option if you want to add Environment Variables as part of configMap. You can provide Environment Variable in key-value pairs, which can be seen and accessed inside a pod.\n Data Volume  Select this option if you want to add Data Volume to your container using Config Map.\nKey-value pairs that you provide here, are provided as a file to the mount path. Your application will read this file as it reads the environment file.\n4. Data In the Data section, you provide your configmap in key-value pairs. You can provide one or more than one environment variable.\nYou can provide variables in two ways-\n  YAML(raw data)\n  GUI (more user friendly)\n  Once you have provided the config, You can click on any option-YAML or GUI to view the key and Value parameters of the ConfigMap.\nKubernetes ConfigMap using Environment Variable:\nIf you select Environment Variable in 3rd option, then you can provide your environment variables in key-value pairs in the Data section using YAML or GUI.\nData in YAML (please Check below screenshot)\n    Now, Click on Save ConfigMap to save your configmap configuration.\nKubernetes ConfigMap using Data Volume:\n    Volume Mount Path Provide the Volume Mount folder path in Volume Mount Path, a path where the data volume needs to be mounted, which will be accessible to the Containers running in a pod.\nYou can add Configuration data as in YAML or GUI format as explained above.\nYou can click on “YAML” or “GUI” to view the key and Value parameters of the ConfigMap that you have created.\nYou can click on Save ConfigMap to save the configMap.\nKubernetes External ConfigMap Kubernetes External ConfigMap is created using the “kubectl create configmap” command. You can also use the ConfigMap generator in kustomization.yaml to create a ConfigMap.\nIf you are using Kubernetes External ConfigMap, make sure you give the name of ConfigMap the same as the name that you have given using kubectl create Configmap command, otherwise, it might result in an error during the built.\nYou have to ensure that the External ConfigMap exists and is available to the pod.\n   The config map is created.\n  Update ConfigMap You can update your configmap anytime later but you cannot change the name of your configmap. If you want to change the name of the configmap then you have to create a new configmap. To update configmap, click on the configmap you have created make changes as required.\n    Delete ConfigMap You can delete your configmap. Click on your configmap and click on the delete sign to delete your configmap.\n    "
},
{
	"uri": "/global_configurations/user_access/",
	"title": "User Access",
	"tags": null,
	"description": "",
	"content": "User/Group Authorization     Authorization is used to determine what functions, data, or other parts of an application the user or the group has the access to.\nYou can manage the User and Group access to Projects, Applications, Chart Groups, Environments, and Roles using the User Access feature.\nView Access Levels/Role\nThere are four different view access levels/Role available for both User and Group, namely:\n  View Only : User(s)/Group(s) can view only selected applications.\n  Build and Deploy : User(s)/Group(s) can build and deploy applications on selected environments.\n  Admin : User(s)/Group(s) can view trigger and edit selected applications.\n  Manager : User(s)/Group(s) can view, trigger and edit selected applications, can also manage user access.\n  To control the access of User and Group,\nGo to the left main panel -\u0026gt; Select Global Configurations -\u0026gt; Select User Access\nUsers 1. Add new user Click on Add New User, to add one or multiple users.\n    2. Create User Permissions If you do not wish to give the users super admin permissions, then control their access in Direct Permissions section. Manage the project, Environment, Application and Role access given to the users.\n    You can add multiple rows, for Direct Permissions.\nOnce you have finished assigning the appropriate permissions for the listed users, Click on Save.\n3. Edit User Permissions You can edit the user permissions, by clicking on the downward arrow.\n    You can then edit the user permissions here.\n    After you have done editing the user permissions. Click on Save.\nIf you want to delete the user/users with particular permissions. Click on Delete.\n4. Manage Chart Group Permissions You can also manage the access of users to Chart Groups in your project.\nYou can either give the users permission to Create or Edit.\nClick on the check box of Create, if you want users to create, view, edit or delete chart groups.\n    Click on the checkbox of Edit, if you want to allow or deny users to edit the chart groups.\nSelect on Deny option from the drop-down menu, if you want to restrict the users to edit the chart groups.\n    Select Specific Charts option from the drop-down menu, and then select the chart groups for which you want to allow users to edit, from the other drop-down menu.\n    Once you have configured all the required permissions for the users, Click on Save.\nGroups The advantage of the groups is to define a set of privileges like create, edit or delete for the given set of resources that can be shared among the users within the group. Users can be added to an existing group to utilize the privileges that it grants.\n1. Add new Group Click on Add Group, to create a new group.\n    Enter the Group Name and Description.\n    2. Create Group Permissions Once you have given the group name and group description.\nThen, control the access permissions of groups in Direct Permissionssection. Manage the project, Environment, Application and Role access given to the groups.\n    You can add multiple rows, for Direct Permissions section.\nOnce you have finished assigning the appropriate permissions for the listed users, Click on Save.\n3. Edit Group Permissions You can edit the group permissions, by clicking on the downward arrow.\n    You can then edit the user permissions here.\n    After you have done editing the group permissions. Click on Save.\nIf you want to delete the groups with particular permissions. Click on Delete.\n4. Manage Chart Group Permissions The chart group permissions for the group will be managed in the same way as for the users. For reference, check Manage chart group permissions for users\n"
},
{
	"uri": "/global_configurations/manage_notification/",
	"title": "Manage Notification",
	"tags": null,
	"description": "",
	"content": "Notification This feature helps you manage the notifications for your build and deployment pipelines. You can recieve the notifications on Slack or via e-mail.\nClick on Global Configurations-\u0026gt; Notification\nManage Configuration Click on Configurations to manage SES Configurations or Slack Configurations\n    Manage SES Configurations You can manage the SES configuration to recieve e-mails by entering the valid credentials. Make sure your e-mail is verified by SES.\n   Click on Add and configure SES.\n       Key Description     Configuation Name Name of the SES Configuration   Access Key ID Valid AWS Access Key ID   Secret Access Key Valid AWS Secret Access Key   AWS Region Select the AWS Region from the drop-down menu   E-mail Enter the SES verified e-mail id on which you wish to recieve e-mail notifications     Manage Slack Configurations You can manage the Slack configurations to recieve notifications on your preferred Slack channel.\n   Click on Add to add new Slack Channel.\n       Key Description     Slack Channel Name of the Slack channel on which you wish to recieve notifications.   Webhook URL Enter the valid Webhook URL link   Project Select the project name to control user access     Manage Notifications Click on Add New to recieve new notification.\n    Manage Slack Notifications     Send To\nFirst, enter the name of your Slack Channel if you have already configured Slack Channel.\nIf you have not yet configured the Slack Channel, Click on Configure Slack Channel\nSelect Pipelines\n  Then, to fetch pipelines of an application, project and environment.\n  Choose a filter type(environment, project or application)\n  Then you will see a list of pipelines, you can select any number of pipelines. For each pipeline there are 3 types of events Trigger, Success and Failure. Click on the check boxes for the events, you wish to recieve notifications.\n       Click on Save after you have configured Slack notifications.\nManage SES Notifications     Send To\n First, enter the e-mail address/addresses on which you want to send e-mail notifications. Make sure e-mail id\u0026rsquo;s are SES Verified.  If you have not yet configured SES, Click on Configure SES\nSelect Pipelines\n  Then, to fetch pipelines of an application, project and environment.\n  Choose a filter type(environment, project or application)\n  Then you will see a list of pipelines, you can select any number of pipelines. For each pipeline there are 3 types of events Trigger, Success and Failure. Click on the check boxes for the events, you wish to recieve notifications.\n       Click on Save after you have configured the e-mail notification.\n"
},
{
	"uri": "/namespaces_and_environments/",
	"title": "Namespaces And Environments",
	"tags": null,
	"description": "",
	"content": "Namespaces Kubernetes namespaces can be seen as a logical entity used to represent cluster resources for usage of a particular set of users. This logical entity can also be termed as a virtual cluster. One physical cluster can be represented as a set of multiple such virtual clusters (namespaces).\nMultiple Namespaces Namespaces are intended for use in environments with many users spread across multiple teams, or projects. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces can not be nested inside one another and each Kubernetes resource can only be in one namespace\nEnvironments One of the advantages that Kubernetes provides is the ability to manage various environments easier and better than traditional deployment strategies. For most nontrivial applications, you have test, staging, and production environments. You can spin up a separate cluster of resources, such as VMs, with the same configuration in staging and production, but that can be costly and managing the differences between the environments can be difficult. Kubernetes includes a cool feature called namespaces, which enables you to manage different environments within the same cluster. For example, you can have different test and staging environments in the same cluster of machines, potentially saving resources.\nEnvironments in Devtron can be accessed from:-\nGlobal Configuration-\u0026gt;Clusters \u0026amp; Environments\n    Here multiple environments can be created\n"
},
{
	"uri": "/creating_application/secrets/",
	"title": "Secrets",
	"tags": null,
	"description": "",
	"content": "Secrets and configmaps both are used to store environment variables but there is only one major difference between them, Configmap stores key-values in normal text format, and secrets store them in base64 encrypted form. Devtron platform hides the data of secrets for the normal users and it is only visible to the users having edit permission.\nSecret objects let you store and manage sensitive information, such as- passwords, auth tokens, and ssh keys. Embedding this information in a secret is safer and more flexible than putting it verbatim in a Pod definition or in a container image.\n    Click on Add Secret to add a new secret.\nConfigure Secret        Key Description     Name Provide a name to your Secret   Data Type Provide the Data Type of your secret. To know about different Data Types available click on Data Types   Data Volume Specify, if there is a volume that is accessible to Containers running in a pod needs to be added.   Use configmap as Environment Variable Select this option if you want to inject Environment Variables in pods using ConfigMap.   Use configmap as Data Volume Select this option, if you want to configure any Data Volume that is accessible to Containers running in a pod and provide a Volume mount path.   Key-Value Provide the key and corresponding value of the provided key.    Data Types There are five Data types that you can use to save your secret.\n  Kubernetes Secret: The secret that you create using Devtron.\n  Kubernetes External Secret: The secret data of your application is fetched externally, converts the Kubernetes External Secret to Kubernetes Secret. The conversion is completely transparent to Pods and secrets are accessed normally.\n  AWS Secret Manager: The secret data of your application is fetched from AWS Secret Manager, converts AWS Secret to Kubernetes Secret. The conversion is completely transparent to Pods that can access secrets normally.\n  AWS System Manager: The secret data for your application is fetched from AWS Secret Manager, converts the secrets stored in AWS System Manager to Kubernetes Secret. The conversion is completely transparent to Pods that can access secrets normally.\n  Hashi Corp Vault: The secret data for your application is fetched from AWS Secret Manager, converts the secrets stored in Harshi Corp Vault to Kubernetes Secret. The conversion is completely transparent to Pods that can access secrets normally.\n  Volume Mount Path Specify the Volume Mount folder path in Volume Mount Path, a path where the data volume needs to be mounted. This volume will be accessible to the Containers running in a pod.\n    Click on Save Secret to save the secret.\n   You can see the Secret is added.\n Update Secrets You can update your secrets anytime later, but you cannot change the name of your secrets. If you want to change your name of secrets then you have to create a new secret.\nTo update secrets, click on the secret you wish to update.\n   Click on Update Secret to update your secret.\nDelete Secret You can delete your secret. Click on your secret and click on the delete sign to delete your secret.\n    "
},
{
	"uri": "/creating_application/environment_overrides/",
	"title": "Environment Overrides",
	"tags": null,
	"description": "",
	"content": "You will see all your environments associated with an application under the Environment Overrides section.\n    You can customize the Deployment template, ConfigMap, Secrets in Environment Overrides section to customize things according to multiple environments such as dev, test, integration, prod, etc.\nDeployment template If you want to deploy an application in a non-production environment and then in production env once testing is done in non-prod env, then you do not have to create a new application for prod env. Your existing pipeline(non-prod env) will work for both the environments with little customization in your deployment template under Environment overrides.\nExample of such customization requirement: In a Non-production environment, you may have specified 100m CPU resources in the deployment template but in the Production environment you may want to have 500m CPU resources as the traffic on Pods will be higher than traffic on non-prod env.\nConfiguring the Deployment template inside Environment overrides will not affect the other environments because Environment Overrides configure deployment template on environment bases. And at the time of deployment, it will always pick the overridden deployment template.\nThe changed configuration will not be added in the template, instead, it will make a copy of the template and lets you customize it and then save it. And now this overridden template will be used for your other Environment.\nClick on Allow Override and make changes to your Deployment template and click on Save to save your changes of the Deployment template.\nConfigMaps \u0026amp; Secrets The same goes for ConfigMap and Secrets. You can also create an environment-specific configmap and Secrets inside the Environment override section.\nIf you want to configure your ConfigMap and secrets at the application level then you can provide them in ConfigMaps and Secrets, but if you want to have environment-specific ConfigMap and secrets then provide them under the Environment override Section. At the time of deployment, it will pick both of them and provide them inside your cluster.\nClick on Update ConfigMap to update Configmaps.\nClick on Update Secrets to update Secrets.\n "
},
{
	"uri": "/security_features/",
	"title": "Security Features",
	"tags": null,
	"description": "",
	"content": " Devtron’s tool is also providing you Security Features to identify the vulnerabilities inside your code and to protect your code from external attacks.\nThe system will scan your code and inform you if there are any Vulnerabilities present in your code. Also to make this feature more flexible to use, we have added a capability using which you can whitelist or block any vulnerability, and your code will be scanned considering the defined whitelist or blocked vulnerabilities.\nRemember, we discussed the Scan for vulnerabilities option in the CI pipeline. You can enable this feature from the CI Pipeline page. The system will scan your code and will show you all vulnerabilities present in your code.\nWe have created Security features to identify the vulnerabilities inside your code and to protect you from external attacks.\nThis Security Feature has two processes:\n  Scanning\n  Policy\n  Scanning This process starts executing after the successful execution of the CI pipeline and before the deployment(CD) process starts.\nIt scans your code to find any potential threat and shows you the list of vulnerabilities as an output of the CI pipeline if it finds any.\nWe will discuss later how you will see the list of your found vulnerabilities.\n Policy Vulnerabilities have different levels like Critical, Moderate, and Low. Users can define Policy according to the level of vulnerability. Users can also block the vulnerability or allow(whitelist) the vulnerability for their code.\nIf any vulnerability is found which is blocked by the user, then it will not deploy the application. And if it finds any vulnerability which is whitelisted by the user, then the build image can be deployed.\nThe user gets informed in both cases if it finds any vulnerability or doesn\u0026rsquo;t find any.\n How to Check Vulnerability\nYou can find the Vulnerabilities Build History Page if you have enabled the Scan for vulnerabilities option.\n    Your Application-\u0026gt; Build History-\u0026gt; Select pipeline-\u0026gt; Go to Security Tab.\nHere you can see all the vulnerabilities found in the build image.\nEvery vulnerability has CVE ID, Severity Level, Package, Current Version, and Fixed In Version.\nCVE ID- Common vulnerability ID\nSeverity Level informs you about the severity of the vulnerability, it is defined as Critical, Medium, and Low.\nThe Package column contains some meta-data of vulnerability.\nThe Current Version is the version of that vulnerability\nFixed In Version column contains version name if it has been fixed in any version, else it remains blank\n Find Vulnerabilities on the Trigger Page\n You can find Vulnerabilities on the Trigger page also. Image having vulnerabilities will be marked as Security Issues Found and you won’t be able to select the image to deploy it.\nYou can see details of these vulnerabilities by expanding the Show Source Info.\nSee the below image.\n    Click on the Show Source Info option. A window will be expanded with two options- Changes and Security. Click on the Security tab to view details about the vulnerabilities in the code.\n   Find Vulnerabilities on the App Details Page\nYou can find Vulnerabilities on the App Details page too. Here we are displaying the total number of vulnerabilities found in the code and their Severity Level wise segregation.\n    Security You can check Vulnerabilities for all your applications in one place. On the Home page, there is an option named as Security. Here, you can see a list of applications under the Security Scan tab. Here all the applications are listed which have the Scan for Vulnerabilities feature enabled. You can see the vulnerability count along with the Severity Level for all your applications.\nNote:-\nIt displays the “Vulnerability count and Severity Level” on a priority basis. And critical level has the highest priority, so it displays the critical level vulnerabilities and there counts if any application is having critical Vulnerability in it.\nYou can directly Search your application using the Search bar or you can filter out your requirement according to Severity, Clusters, and Environment.\n    Now if you click on the severity level of vulnerability it will show you the list of all vulnerabilities along with other details.\n    Security Policies: Users can define Security policies for their vulnerabilities under Security Policies Tab.\nHome Page-\u0026gt; Security - \u0026gt; Security Policies\nPolicies can be defined to different levels-\n  Global\n  Cluster\n  Environment\n  Application\n  Note:-\nPolicies work in hierarchical order.\nOrder to be followed- First Global and second Cluster and so on as you can see the order of the options\n    Some examples of how policies can be defined\nUsers can block all the critical vulnerabilities and allow the moderate and low vulnerabilities or Users can block all vulnerabilities or users can block all vulnerabilities for one application and can block only Critical vulnerabilities for other applications.\nConfigure Global Policy To configure these policies, click on the drop-down option of the severity levels and select Block or Allow.\n    Configure Cluster Security Policy In the Global Security Policies, there are only two options available- Block and Allow. But in other options, we have an extra option named Inherit.\nAs the name itself gives you an idea about this option, it fetches the policy of its upper-level options, if we choose to inherit in the drop-down.\nExample-if you block critical severity levels in Global, then critical levels will be blocked in Cluster Security Policy. In case we change critical policy globally and allow it there, then it will be allowed in Cluster Security Policies too. But you can change these policies explicitly.\nIf you want to block Critical Vulnerabilities in Global Security Policies but want to allow them in some clusters, then select your cluster and change the critical drop-down to allow. It will not affect the policy of other clusters and global also.\nConfigure Environment Security Policy Again we have three options to define a policy- Block, Allow, and Inherit.\nEnvironment Security Policy inherits the policy from Cluster Security Policy. Each level inherits the policy of its upper level.\nSelect any environment here, you will find it is inheriting the policy of Cluster.\nExample- If you have blocked critical level vulnerabilities in Global Security Policy but allowed them in Cluster Security Policy, then Environment Security Policy will inherit the policy of cluster not global, Hence critical level vulnerabilities will be allowed in the Environment Security Policy.\nThough, You can change the policy explicitly.\nConfigure Application Security Policy The same thing goes with the Application Security Policy. But in Application, the policy is set with the combination of Environment option and Application option. If you change the policy in a dev environment that it will apply to all the applications which are in the dev environment.\nCheck CVE Policy Here is the last option Check CVE Policy, If you want to configure a security policy specific to any Vulnerability, you can use this option.\nClick on this option, it will show you a search bar, copy any CVE ID or vulnerability ID, and click on Search. It will display the details regarding that CVE ID and you can configure the policy to that particular CVE ID.\n"
},
{
	"uri": "/deleting_applications/",
	"title": "Deleting Application",
	"tags": null,
	"description": "",
	"content": "Delete the Application, when you are sure you no longer need it.\nClicking on Delete Application will not delete your application if you have workflows in the application.\nIf your Application contains workflows in the Workflow Editor. So,when you Click on Delete Application, you will see the following prompt.\n    Click on View Workflows to view and delete your workflows in the application.\nTo delete the workflows in your application, you must first delete all the pipelines(CD Pipeline, CI Pipeline or Linked CI Pipeline or External CI Pipeline if there are any).\n    After you have deleted all the pipelines in the workflow, you can delete that particular workflow.\nSimilarly, delete all the workflows in the application.\n    Now, Click on Delete Application to delete the application.\n"
},
{
	"uri": "/global_configurations/",
	"title": "Global Configurations",
	"tags": null,
	"description": "",
	"content": "This documentation consists of the Global Configurations available in Devtron.\nParts of the Documentation\nGit Accounts\nDocker Registries\nCluster And Environments\nProjects\nUser Access\nManage Notification\n"
},
{
	"uri": "/use_cases/",
	"title": "Use Cases",
	"tags": null,
	"description": "",
	"content": "Welcome, this document consists of Devtron Use Cases\nRun Cron Jobs or One Time Job using Devtron Generic Helm Chart\n Connect SpringBoot with Mysql Database\n Connect Expressjs With Mongodb Database\n Connect Django With Mysql Database\n"
},
{
	"uri": "/docs/",
	"title": " ",
	"tags": null,
	"description": "",
	"content": ""
},
{
	"uri": "/docs/hidden/",
	"title": "",
	"tags": null,
	"description": "",
	"content": ""
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": null,
	"description": "",
	"content": ""
},
{
	"uri": "/",
	"title": "Introduction",
	"tags": null,
	"description": "",
	"content": "Devtron Documentation Continuous Deployment for Kubernetes that increases your productivity and reduces cost.\nBrowse Devtron Documentation\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": null,
	"description": "",
	"content": ""
}]
